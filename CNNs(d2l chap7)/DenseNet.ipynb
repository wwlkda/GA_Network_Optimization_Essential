{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import datasets,transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "from IPython import display\n",
    "from torch.functional import F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_channels,output_channels):\n",
    "    return nn.Sequential(nn.BatchNorm2d(input_channels),nn.ReLU(),\n",
    "                         nn.Conv2d(input_channels,output_channels,kernel_size=3,padding=1))\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self,num_convs,input_channels,num_channels):\n",
    "        super().__init__()\n",
    "        layer=[]\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block(num_channels*i+input_channels,num_channels))\n",
    "        self.net=nn.Sequential(*layer)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        for blk in self.net:\n",
    "            Y=blk(X)\n",
    "            X=torch.cat((X,Y),dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk=DenseBlock(2,3,10)\n",
    "X=torch.randn(4,3,8,8)\n",
    "Y=blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(input_channels,num_channels):\n",
    "    return nn.Sequential(nn.BatchNorm2d(input_channels),nn.ReLU(),\n",
    "                         nn.Conv2d(input_channels,num_channels,kernel_size=1),\n",
    "                         nn.AvgPool2d(kernel_size=2,stride=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk=transition_block(23,10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=nn.Sequential(nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),\n",
    "                 nn.BatchNorm2d(64),nn.ReLU(),\n",
    "                 nn.MaxPool2d(kernel_size=3,stride=2,padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels,growth_rate=64,32\n",
    "num_convs_in_dense_blocks=[4,4,4,4]\n",
    "blks=[]\n",
    "for i,num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    blks.append(DenseBlock(num_convs,num_channels,growth_rate))\n",
    "    num_channels+=num_convs*growth_rate\n",
    "    if i!=len(num_convs_in_dense_blocks)-1:#稠密块之间插入过渡块\n",
    "        blks.append(transition_block(num_channels,num_channels//2))\n",
    "        num_channels=num_channels//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(b1,*blks,\n",
    "                  nn.BatchNorm2d(num_channels),nn.ReLU(),\n",
    "                  nn.AdaptiveMaxPool2d((1,1)),\n",
    "                  nn.Flatten(),\n",
    "                  nn.Linear(num_channels,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "DenseBlock output shape:\t torch.Size([1, 192, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 96, 12, 12])\n",
      "DenseBlock output shape:\t torch.Size([1, 224, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 112, 6, 6])\n",
      "DenseBlock output shape:\t torch.Size([1, 240, 6, 6])\n",
      "Sequential output shape:\t torch.Size([1, 120, 3, 3])\n",
      "DenseBlock output shape:\t torch.Size([1, 248, 3, 3])\n",
      "BatchNorm2d output shape:\t torch.Size([1, 248, 3, 3])\n",
      "ReLU output shape:\t torch.Size([1, 248, 3, 3])\n",
      "AdaptiveMaxPool2d output shape:\t torch.Size([1, 248, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 248])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X=torch.rand(size=(1,1,96,96))\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da261d0780e93bc3d36c3a39c2e49d7a3bd7850c9b78b44ae2ab8b819bf79433"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('MyEnviron': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
